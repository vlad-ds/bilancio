# Plan 025: Cloud Storage for Experiment Results

## Overview

This plan outlines the architecture for storing bilancio experiment results in external cloud storage, enabling persistent storage, cross-machine access, and potential collaboration. The focus is on free or minimal-cost solutions.

## Current State Analysis

### Data Generated by Complex Runs

**Per-Run Data (Control/Treatment or Passive/Active):**
| File | Size | Content |
|------|------|---------|
| `events.jsonl` | 50KB | Event log (300-400 lines) |
| `balances.csv` | 5KB | Per-agent balance snapshots |
| `metrics.csv` | 250B | Summary metrics |
| `metrics_ds.csv` | 2.3KB | Day-by-day statistics |
| `dealer_metrics.json` | Variable | Dealer performance |
| `trades.csv` | Variable | Trade records (detailed mode) |
| `run.html` | 1MB+ | HTML transcript |

**Aggregate Data (Most Valuable for Analysis):**
| File | Size | Content |
|------|------|---------|
| `comparison.csv` | 10-50KB | Paired results (one row per parameter combo) |
| `summary.json` | 2KB | Sweep statistics and config |
| `registry/experiments.csv` | 15-30KB | Per-branch run index |

### Current Data Volumes

| Experiment Type | Pairs | Total Runs | Size |
|-----------------|-------|------------|------|
| plan024_sweep (aggregates only) | 125 | 250 | 47KB |
| detailed_instrumentation | 625 | 1,250 | 114MB |
| balanced_comparison_20251204 | 125 | 250 | 128MB |

### Projected Usage

- **Regular research:** 2-5 sweeps/month
- **Per sweep (aggregates):** ~50KB
- **Per sweep (full data):** ~100-150MB
- **Monthly aggregate storage:** ~250KB (negligible)
- **Monthly full storage:** ~500MB-750MB
- **Annual projection:** ~6-9GB full data, ~3MB aggregates

## Recommended Architecture

### Tier 1: Essential (Aggregates Only) - FREE

Store only the high-value aggregate data that enables analysis and reproducibility.

**Data to Store:**
- `comparison.csv` - All paired results
- `summary.json` - Configuration and statistics
- `registry/experiments.csv` - Run index for both branches

**Recommended Service: Supabase PostgreSQL (Free Tier)**

```
Storage: 500MB (enough for years of aggregate data)
Bandwidth: 5GB/month
Cost: $0/month
```

**Schema Design:**
```sql
-- Sweep metadata
CREATE TABLE sweeps (
    sweep_id UUID PRIMARY KEY,
    name TEXT NOT NULL,
    sweep_type TEXT NOT NULL,  -- 'balanced_comparison', 'ring_sweep', etc.
    created_at TIMESTAMP DEFAULT NOW(),
    config JSONB NOT NULL,
    summary JSONB,
    pairs_total INT,
    pairs_completed INT
);

-- Comparison results
CREATE TABLE comparison_results (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    sweep_id UUID REFERENCES sweeps(sweep_id),
    -- Parameters
    kappa DECIMAL,
    concentration DECIMAL,
    mu DECIMAL,
    monotonicity DECIMAL,
    seed INT,
    face_value DECIMAL,
    outside_mid_ratio DECIMAL,
    -- Passive metrics
    delta_passive DECIMAL,
    phi_passive DECIMAL,
    passive_run_id TEXT,
    passive_status TEXT,
    -- Active metrics
    delta_active DECIMAL,
    phi_active DECIMAL,
    active_run_id TEXT,
    active_status TEXT,
    -- Derived
    trading_effect DECIMAL,
    trading_relief_ratio DECIMAL,
    -- Dealer metrics
    dealer_total_pnl DECIMAL,
    dealer_total_return DECIMAL,
    total_trades INT
);

-- Run registry
CREATE TABLE run_registry (
    run_id TEXT PRIMARY KEY,
    sweep_id UUID REFERENCES sweeps(sweep_id),
    branch TEXT,  -- 'passive', 'active', 'control', 'treatment'
    parameters JSONB,
    status TEXT,
    created_at TIMESTAMP DEFAULT NOW()
);
```

### Tier 2: Full Data Storage (Per-Run Files) - FREE to LOW COST

**Option A: Cloudflare R2 (RECOMMENDED)**
```
Free Tier:
- 10GB storage
- 1 million Class A operations (write)
- 10 million Class B operations (read)
- ZERO egress fees (unique advantage)

Cost beyond free tier:
- $0.015/GB/month storage
- $4.50/million Class A ops
- $0.36/million Class B ops
```

**Estimated Cost for bilancio:**
- Year 1 (9GB): FREE
- Year 2 (18GB): ~$0.12/month ($1.44/year)

**Option B: Backblaze B2**
```
Free Tier:
- 10GB storage
- Free egress to Cloudflare (via bandwidth alliance)

Cost beyond free tier:
- $0.005/GB/month (cheapest raw storage)
- $0.01/GB egress (without Cloudflare)
```

**Option C: AWS S3 (Free Tier - 12 months only)**
```
Free Tier (first 12 months):
- 5GB storage
- 20,000 GET requests
- 2,000 PUT requests

After free tier:
- $0.023/GB/month (standard)
- $0.004/GB/month (Glacier Instant Retrieval)
```

### Recommended Hybrid Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│                        bilancio sweep                          │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│                    Local File System                            │
│  out/experiments/{sweep_name}/                                  │
│  ├── passive/ or control/                                       │
│  │   ├── registry/experiments.csv                               │
│  │   └── runs/{run_id}/...                                      │
│  ├── active/ or treatment/                                      │
│  └── aggregate/                                                 │
│      ├── comparison.csv                                         │
│      └── summary.json                                           │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
              ┌───────────────┴───────────────┐
              │         Cloud Sync            │
              ▼                               ▼
┌──────────────────────────┐   ┌──────────────────────────────────┐
│    Supabase (FREE)       │   │     Cloudflare R2 (FREE)         │
│    PostgreSQL            │   │     Object Storage               │
│                          │   │                                  │
│  • sweeps table          │   │  • /bilancio-experiments/        │
│  • comparison_results    │   │    ├── {sweep_name}/             │
│  • run_registry          │   │    │   ├── aggregate.tar.gz      │
│                          │   │    │   ├── passive_runs.tar.gz   │
│  ~3MB for years of data  │   │    │   └── active_runs.tar.gz    │
│  SQL queryable           │   │                                  │
│  REST/GraphQL API        │   │  Compressed: ~50% size reduction │
└──────────────────────────┘   │  Zero egress fees                │
                               └──────────────────────────────────┘
```

## Implementation Plan

### Phase 1: Supabase Integration (Aggregates)

**1.1 Create Supabase Project**
- Sign up at supabase.com
- Create new project (free tier)
- Get connection string and API keys

**1.2 Add Dependencies**
```toml
# pyproject.toml
[project.optional-dependencies]
cloud = [
    "supabase>=2.0.0",
    "boto3>=1.34.0",  # For S3-compatible storage
]
```

**1.3 Create Storage Module**

New file: `src/bilancio/storage/__init__.py`
```python
"""Cloud storage integration for bilancio experiments."""

from .supabase_client import SupabaseStorage
from .r2_client import R2Storage
from .sync import sync_sweep_to_cloud

__all__ = ["SupabaseStorage", "R2Storage", "sync_sweep_to_cloud"]
```

New file: `src/bilancio/storage/supabase_client.py`
```python
"""Supabase PostgreSQL integration for aggregate experiment data."""

import os
import json
from pathlib import Path
from typing import Optional, Dict, Any, List
from decimal import Decimal
import csv

from supabase import create_client, Client


class SupabaseStorage:
    """Client for storing experiment aggregates in Supabase."""

    def __init__(self, url: Optional[str] = None, key: Optional[str] = None):
        self.url = url or os.environ.get("SUPABASE_URL")
        self.key = key or os.environ.get("SUPABASE_KEY")

        if not self.url or not self.key:
            raise ValueError(
                "Supabase credentials required. Set SUPABASE_URL and SUPABASE_KEY "
                "environment variables or pass them to constructor."
            )

        self.client: Client = create_client(self.url, self.key)

    def upload_sweep(
        self,
        sweep_name: str,
        sweep_type: str,
        comparison_csv_path: Path,
        summary_json_path: Path,
    ) -> str:
        """Upload sweep aggregate data to Supabase.

        Returns:
            sweep_id: UUID of the created sweep record
        """
        # Load summary.json
        with open(summary_json_path) as f:
            summary = json.load(f)

        # Create sweep record
        sweep_data = {
            "name": sweep_name,
            "sweep_type": sweep_type,
            "config": summary.get("config", {}),
            "summary": {k: v for k, v in summary.items() if k != "config"},
            "pairs_total": summary.get("total_pairs"),
            "pairs_completed": summary.get("completed_pairs"),
        }

        result = self.client.table("sweeps").insert(sweep_data).execute()
        sweep_id = result.data[0]["sweep_id"]

        # Load and upload comparison results
        comparison_rows = []
        with open(comparison_csv_path) as f:
            reader = csv.DictReader(f)
            for row in reader:
                comparison_rows.append({
                    "sweep_id": sweep_id,
                    **{k: self._convert_value(v) for k, v in row.items()}
                })

        # Batch insert comparison results
        if comparison_rows:
            self.client.table("comparison_results").insert(comparison_rows).execute()

        return sweep_id

    def _convert_value(self, value: str) -> Any:
        """Convert CSV string values to appropriate types."""
        if value == "" or value is None:
            return None
        try:
            if "." in value or "E" in value or "e" in value:
                return float(value)
            return int(value)
        except ValueError:
            return value

    def query_sweeps(
        self,
        sweep_type: Optional[str] = None,
        limit: int = 100,
    ) -> List[Dict[str, Any]]:
        """Query sweep metadata."""
        query = self.client.table("sweeps").select("*")

        if sweep_type:
            query = query.eq("sweep_type", sweep_type)

        return query.limit(limit).execute().data

    def query_comparison_results(
        self,
        sweep_id: str,
        filters: Optional[Dict[str, Any]] = None,
    ) -> List[Dict[str, Any]]:
        """Query comparison results for a sweep."""
        query = self.client.table("comparison_results").select("*").eq("sweep_id", sweep_id)

        if filters:
            for key, value in filters.items():
                query = query.eq(key, value)

        return query.execute().data
```

**1.4 Create R2/S3 Module**

New file: `src/bilancio/storage/r2_client.py`
```python
"""Cloudflare R2 / S3-compatible storage for full experiment data."""

import os
import tarfile
import tempfile
from pathlib import Path
from typing import Optional

import boto3
from botocore.config import Config


class R2Storage:
    """Client for storing full experiment data in R2/S3."""

    def __init__(
        self,
        endpoint_url: Optional[str] = None,
        access_key_id: Optional[str] = None,
        secret_access_key: Optional[str] = None,
        bucket_name: Optional[str] = None,
    ):
        self.endpoint_url = endpoint_url or os.environ.get("R2_ENDPOINT_URL")
        self.access_key_id = access_key_id or os.environ.get("R2_ACCESS_KEY_ID")
        self.secret_access_key = secret_access_key or os.environ.get("R2_SECRET_ACCESS_KEY")
        self.bucket_name = bucket_name or os.environ.get("R2_BUCKET_NAME", "bilancio-experiments")

        if not all([self.endpoint_url, self.access_key_id, self.secret_access_key]):
            raise ValueError(
                "R2 credentials required. Set R2_ENDPOINT_URL, R2_ACCESS_KEY_ID, "
                "and R2_SECRET_ACCESS_KEY environment variables."
            )

        self.client = boto3.client(
            "s3",
            endpoint_url=self.endpoint_url,
            aws_access_key_id=self.access_key_id,
            aws_secret_access_key=self.secret_access_key,
            config=Config(signature_version="s3v4"),
        )

    def upload_sweep(
        self,
        sweep_name: str,
        local_path: Path,
        compress: bool = True,
    ) -> dict:
        """Upload full sweep data to R2.

        Args:
            sweep_name: Name of the sweep (used as prefix in bucket)
            local_path: Local path to sweep directory
            compress: Whether to compress data into tar.gz archives

        Returns:
            Dict with upload statistics
        """
        uploaded = {"files": 0, "bytes": 0}

        if compress:
            # Create compressed archives for each branch
            for branch_name in ["passive", "active", "control", "treatment"]:
                branch_path = local_path / branch_name
                if branch_path.exists():
                    self._upload_compressed(
                        sweep_name, branch_name, branch_path, uploaded
                    )

            # Upload aggregate as-is (small files)
            aggregate_path = local_path / "aggregate"
            if aggregate_path.exists():
                for file in aggregate_path.iterdir():
                    if file.is_file():
                        key = f"{sweep_name}/aggregate/{file.name}"
                        self.client.upload_file(str(file), self.bucket_name, key)
                        uploaded["files"] += 1
                        uploaded["bytes"] += file.stat().st_size
        else:
            # Upload all files directly
            for file in local_path.rglob("*"):
                if file.is_file():
                    key = f"{sweep_name}/{file.relative_to(local_path)}"
                    self.client.upload_file(str(file), self.bucket_name, key)
                    uploaded["files"] += 1
                    uploaded["bytes"] += file.stat().st_size

        return uploaded

    def _upload_compressed(
        self,
        sweep_name: str,
        branch_name: str,
        branch_path: Path,
        stats: dict,
    ) -> None:
        """Create and upload a compressed archive."""
        with tempfile.NamedTemporaryFile(suffix=".tar.gz", delete=False) as tmp:
            tmp_path = Path(tmp.name)

        try:
            with tarfile.open(tmp_path, "w:gz") as tar:
                tar.add(branch_path, arcname=branch_name)

            key = f"{sweep_name}/{branch_name}.tar.gz"
            self.client.upload_file(str(tmp_path), self.bucket_name, key)
            stats["files"] += 1
            stats["bytes"] += tmp_path.stat().st_size
        finally:
            tmp_path.unlink()

    def download_sweep(
        self,
        sweep_name: str,
        local_path: Path,
        decompress: bool = True,
    ) -> dict:
        """Download sweep data from R2.

        Args:
            sweep_name: Name of the sweep
            local_path: Local path to download to
            decompress: Whether to decompress tar.gz archives

        Returns:
            Dict with download statistics
        """
        downloaded = {"files": 0, "bytes": 0}

        # List objects with sweep prefix
        paginator = self.client.get_paginator("list_objects_v2")

        for page in paginator.paginate(Bucket=self.bucket_name, Prefix=f"{sweep_name}/"):
            for obj in page.get("Contents", []):
                key = obj["Key"]
                relative = key[len(sweep_name) + 1:]  # Remove prefix
                local_file = local_path / relative
                local_file.parent.mkdir(parents=True, exist_ok=True)

                self.client.download_file(self.bucket_name, key, str(local_file))
                downloaded["files"] += 1
                downloaded["bytes"] += obj["Size"]

                # Decompress if needed
                if decompress and local_file.suffix == ".gz" and local_file.stem.endswith(".tar"):
                    with tarfile.open(local_file, "r:gz") as tar:
                        tar.extractall(local_file.parent)
                    local_file.unlink()

        return downloaded

    def list_sweeps(self) -> list:
        """List all sweeps in the bucket."""
        sweeps = set()
        paginator = self.client.get_paginator("list_objects_v2")

        for page in paginator.paginate(Bucket=self.bucket_name, Delimiter="/"):
            for prefix in page.get("CommonPrefixes", []):
                sweep_name = prefix["Prefix"].rstrip("/")
                sweeps.add(sweep_name)

        return sorted(sweeps)
```

**1.5 Create Sync Utility**

New file: `src/bilancio/storage/sync.py`
```python
"""Unified sync interface for uploading experiments to cloud storage."""

import logging
from pathlib import Path
from typing import Optional

from .supabase_client import SupabaseStorage
from .r2_client import R2Storage

logger = logging.getLogger(__name__)


def sync_sweep_to_cloud(
    sweep_path: Path,
    sweep_name: Optional[str] = None,
    sweep_type: str = "balanced_comparison",
    upload_full_data: bool = False,
) -> dict:
    """Sync a sweep to cloud storage.

    Args:
        sweep_path: Path to the sweep directory
        sweep_name: Optional name override (defaults to directory name)
        sweep_type: Type of sweep for metadata
        upload_full_data: Whether to upload full per-run data to R2

    Returns:
        Dict with sync results
    """
    sweep_name = sweep_name or sweep_path.name
    results = {"sweep_name": sweep_name, "supabase": None, "r2": None}

    # Upload aggregates to Supabase
    comparison_csv = sweep_path / "aggregate" / "comparison.csv"
    summary_json = sweep_path / "aggregate" / "summary.json"

    if comparison_csv.exists() and summary_json.exists():
        try:
            supabase = SupabaseStorage()
            sweep_id = supabase.upload_sweep(
                sweep_name=sweep_name,
                sweep_type=sweep_type,
                comparison_csv_path=comparison_csv,
                summary_json_path=summary_json,
            )
            results["supabase"] = {"sweep_id": sweep_id, "status": "success"}
            logger.info(f"Uploaded aggregates to Supabase: {sweep_id}")
        except Exception as e:
            results["supabase"] = {"status": "error", "error": str(e)}
            logger.error(f"Failed to upload to Supabase: {e}")
    else:
        results["supabase"] = {"status": "skipped", "reason": "missing aggregate files"}

    # Upload full data to R2 if requested
    if upload_full_data:
        try:
            r2 = R2Storage()
            upload_stats = r2.upload_sweep(sweep_name, sweep_path, compress=True)
            results["r2"] = {"status": "success", **upload_stats}
            logger.info(f"Uploaded full data to R2: {upload_stats}")
        except Exception as e:
            results["r2"] = {"status": "error", "error": str(e)}
            logger.error(f"Failed to upload to R2: {e}")

    return results
```

### Phase 2: CLI Integration

**2.1 Add CLI Commands**

Update `src/bilancio/ui/cli.py`:
```python
@main.group()
def cloud():
    """Cloud storage commands."""
    pass


@cloud.command()
@click.argument("sweep_path", type=click.Path(exists=True, path_type=Path))
@click.option("--name", help="Override sweep name")
@click.option("--type", "sweep_type", default="balanced_comparison", help="Sweep type")
@click.option("--full", is_flag=True, help="Upload full per-run data to R2")
def sync(sweep_path: Path, name: str, sweep_type: str, full: bool):
    """Sync a sweep to cloud storage."""
    from bilancio.storage import sync_sweep_to_cloud

    results = sync_sweep_to_cloud(
        sweep_path=sweep_path,
        sweep_name=name,
        sweep_type=sweep_type,
        upload_full_data=full,
    )

    click.echo(f"Sync results for {results['sweep_name']}:")
    click.echo(f"  Supabase: {results['supabase']}")
    if results['r2']:
        click.echo(f"  R2: {results['r2']}")


@cloud.command()
@click.option("--type", "sweep_type", help="Filter by sweep type")
def list_sweeps(sweep_type: str):
    """List sweeps in cloud storage."""
    from bilancio.storage import SupabaseStorage

    supabase = SupabaseStorage()
    sweeps = supabase.query_sweeps(sweep_type=sweep_type)

    for sweep in sweeps:
        click.echo(f"{sweep['name']} ({sweep['sweep_type']}) - {sweep['pairs_completed']} pairs")


@cloud.command()
@click.argument("sweep_name")
@click.argument("output_path", type=click.Path(path_type=Path))
def download(sweep_name: str, output_path: Path):
    """Download a sweep from cloud storage."""
    from bilancio.storage import R2Storage

    r2 = R2Storage()
    stats = r2.download_sweep(sweep_name, output_path)
    click.echo(f"Downloaded {stats['files']} files ({stats['bytes']} bytes)")
```

### Phase 3: Sweep Runner Integration

**3.1 Add Auto-Sync to BalancedComparisonRunner**

Update `src/bilancio/experiments/balanced_comparison.py`:
```python
class BalancedComparisonConfig(BaseModel):
    # ... existing fields ...

    # Cloud storage options
    cloud_sync_enabled: bool = Field(default=False, description="Auto-sync to cloud")
    cloud_sync_full_data: bool = Field(default=False, description="Upload full per-run data")


class BalancedComparisonRunner:
    def run_sweep(self, ...):
        # ... existing code ...

        # After sweep completion
        if self.config.cloud_sync_enabled:
            from bilancio.storage import sync_sweep_to_cloud

            sync_sweep_to_cloud(
                sweep_path=self.output_dir,
                sweep_type="balanced_comparison",
                upload_full_data=self.config.cloud_sync_full_data,
            )
```

## Cost Analysis

### Scenario: Regular Research Usage

**Assumptions:**
- 4 sweeps/month (2 detailed, 2 aggregate-only)
- 125 parameter pairs per sweep
- 1 year of data retention

### Year 1 Costs

| Service | Data Volume | Cost |
|---------|-------------|------|
| Supabase (aggregates) | ~1MB | FREE |
| Cloudflare R2 (full data) | ~6GB | FREE |
| **Total** | | **$0/month** |

### Year 2+ Costs (if exceeding free tiers)

| Service | Data Volume | Cost |
|---------|-------------|------|
| Supabase | ~2MB | FREE (500MB limit) |
| Cloudflare R2 | ~12GB | ~$0.03/month |
| **Total** | | **~$0.40/year** |

### Scaling Scenarios

| Usage Level | Annual Data | Supabase | R2 | Total/Year |
|-------------|-------------|----------|-----|------------|
| Light (2 sweeps/mo) | 3GB | FREE | FREE | $0 |
| Regular (4 sweeps/mo) | 6GB | FREE | FREE | $0 |
| Heavy (10 sweeps/mo) | 15GB | FREE | $0.90 | $0.90 |
| Research team (50 sweeps/mo) | 75GB | $25 | $11.70 | $36.70 |

## Alternative Options Considered

### Why Not These?

| Option | Reason |
|--------|--------|
| **AWS S3** | Egress fees add up; free tier only 12 months |
| **Google Cloud Storage** | Similar egress concerns |
| **MongoDB Atlas** | 512MB free tier too small for full data |
| **Firebase** | Not suited for large file storage |
| **GitHub LFS** | 1GB free limit, then expensive |
| **DuckDB + Parquet** | Good for analysis, but no native cloud sync |

### When to Consider Alternatives

- **AWS S3**: If you need AWS Lambda integration
- **Backblaze B2**: If R2 has region availability issues
- **Self-hosted MinIO**: If you need on-premise solution
- **SQLite + Litestream**: If you want simpler replication

## Environment Configuration

```bash
# .env file (do not commit!)

# Supabase (free tier)
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_KEY=your-anon-key

# Cloudflare R2 (free tier)
R2_ENDPOINT_URL=https://your-account-id.r2.cloudflarestorage.com
R2_ACCESS_KEY_ID=your-access-key
R2_SECRET_ACCESS_KEY=your-secret-key
R2_BUCKET_NAME=bilancio-experiments
```

## Security Considerations

1. **Never commit credentials** - Use environment variables or `.env` file
2. **Use Row Level Security (RLS)** in Supabase for multi-tenant access
3. **Create separate API keys** for CI/CD vs local development
4. **Enable bucket versioning** in R2 for data recovery
5. **Set up lifecycle policies** to auto-delete old data if needed

## Files to Create/Modify

**New Files:**
- `src/bilancio/storage/__init__.py`
- `src/bilancio/storage/supabase_client.py`
- `src/bilancio/storage/r2_client.py`
- `src/bilancio/storage/sync.py`
- `tests/unit/test_storage.py`

**Modified Files:**
- `pyproject.toml` (add cloud dependencies)
- `src/bilancio/ui/cli.py` (add cloud commands)
- `src/bilancio/experiments/balanced_comparison.py` (auto-sync option)
- `.gitignore` (add .env)

## Implementation Order

1. **Phase 1**: Supabase schema + client (2-3 hours)
2. **Phase 2**: R2 client + compression (2-3 hours)
3. **Phase 3**: Sync utility + CLI (1-2 hours)
4. **Phase 4**: Sweep runner integration (1 hour)
5. **Phase 5**: Tests + documentation (2 hours)

## Success Criteria

- [ ] `bilancio cloud sync out/experiments/plan024_sweep` uploads aggregates to Supabase
- [ ] `bilancio cloud sync --full out/experiments/plan024_sweep` also uploads to R2
- [ ] `bilancio cloud list-sweeps` shows all uploaded sweeps
- [ ] `bilancio cloud download sweep_name ./local_path` restores data
- [ ] Auto-sync works when `cloud_sync_enabled=True` in sweep config
- [ ] Total cost remains $0/month for regular usage
